name: Indeed Costa Rica Job Scraper

on:
  # Run on manual trigger
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '5'
      max_jobs:
        description: 'Maximum jobs to scrape (leave empty for no limit)'
        required: false
        default: ''
  
  # Run on schedule (every day at 9 AM UTC)
  schedule:
    - cron: '0 9 * * *'
  
  # Run on push to main branch (optional)
  push:
    branches:
      - main
    paths:
      - 'indeed_full_details_scraper.py'
      - '.github/workflows/indeed_scraper.yml'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          wget \
          gnupg \
          ca-certificates \
          fonts-liberation \
          libasound2 \
          libatk-bridge2.0-0 \
          libatk1.0-0 \
          libatspi2.0-0 \
          libcups2 \
          libdbus-1-3 \
          libdrm2 \
          libgbm1 \
          libgtk-3-0 \
          libnspr4 \
          libnss3 \
          libwayland-client0 \
          libxcomposite1 \
          libxdamage1 \
          libxfixes3 \
          libxkbcommon0 \
          libxrandr2 \
          xdg-utils \
          libu2f-udev \
          libvulkan1
    
    - name: Install Chrome
      run: |
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        google-chrome --version
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install undetected-chromedriver selenium
    
    - name: Set environment variables
      run: |
        echo "MAX_PAGES=${{ github.event.inputs.max_pages || '5' }}" >> $GITHUB_ENV
        echo "MAX_JOBS=${{ github.event.inputs.max_jobs || '' }}" >> $GITHUB_ENV
        echo "GITHUB_ACTIONS=true" >> $GITHUB_ENV
    
    - name: Run scraper
      run: |
        python indeed_full_details_scraper.py
      continue-on-error: false
    
    - name: Check output files
      run: |
        echo "Generated files:"
        ls -lh indeed_cr_jobs_*.json indeed_cr_jobs_*.csv 2>/dev/null || echo "No output files found"
    
    - name: Upload JSON results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: indeed-jobs-json-${{ github.run_number }}
        path: indeed_cr_jobs_*.json
        retention-days: 30
        if-no-files-found: warn
    
    - name: Upload CSV results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: indeed-jobs-csv-${{ github.run_number }}
        path: indeed_cr_jobs_*.csv
        retention-days: 30
        if-no-files-found: warn
    
    - name: Commit results to repository (optional)
      if: success()
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Create results directory if it doesn't exist
        mkdir -p results
        
        # Move results to results directory
        mv indeed_cr_jobs_*.json results/ 2>/dev/null || true
        mv indeed_cr_jobs_*.csv results/ 2>/dev/null || true
        
        # Add and commit if there are changes
        git add results/
        git diff --staged --quiet || git commit -m "Add scraped jobs from $(date +'%Y-%m-%d %H:%M:%S')"
        git push || echo "Nothing to push or push failed"
      continue-on-error: true
    
    - name: Create summary
      if: always()
      run: |
        echo "## Scraping Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f indeed_cr_jobs_*.json ] || [ -f results/indeed_cr_jobs_*.json ]; then
          JSON_FILE=$(ls indeed_cr_jobs_*.json results/indeed_cr_jobs_*.json 2>/dev/null | head -1)
          if [ -f "$JSON_FILE" ]; then
            JOB_COUNT=$(python -c "import json; print(len(json.load(open('$JSON_FILE'))))" 2>/dev/null || echo "0")
            echo "✅ Successfully scraped **$JOB_COUNT jobs**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "Files generated:" >> $GITHUB_STEP_SUMMARY
            echo "- JSON: $(basename $JSON_FILE)" >> $GITHUB_STEP_SUMMARY
            CSV_FILE=$(echo $JSON_FILE | sed 's/.json/.csv/')
            if [ -f "$CSV_FILE" ]; then
              echo "- CSV: $(basename $CSV_FILE)" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "⚠️ Scraping completed but no jobs found" >> $GITHUB_STEP_SUMMARY
          fi
        else
          echo "❌ Scraping failed - no output files" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Run completed at: $(date +'%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY

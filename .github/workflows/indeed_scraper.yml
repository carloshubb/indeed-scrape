name: Indeed Costa Rica Job Scraper

on:
  # Run on schedule (daily at 8 AM UTC)
  schedule:
    - cron: '0 8 * * *'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Maximum pages to scrape'
        required: false
        default: '5'
      max_jobs:
        description: 'Maximum jobs to scrape (leave empty for all)'
        required: false
        default: ''
      headless:
        description: 'Run in headless mode'
        required: false
        default: 'true'

jobs:
  scrape-jobs:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip xvfb
        
        # Install Chrome
        wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo apt-get install -y ./google-chrome-stable_current_amd64.deb
        rm google-chrome-stable_current_amd64.deb
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium undetected-chromedriver webdriver-manager
    
    - name: Run scraper
      env:
        MAX_PAGES: ${{ github.event.inputs.max_pages || '5' }}
        MAX_JOBS: ${{ github.event.inputs.max_jobs || '' }}
        HEADLESS: ${{ github.event.inputs.headless || 'true' }}
      run: |
        # Run with xvfb for headless Chrome
        xvfb-run -a python indeed_full_details_scraper.py
    
    - name: Upload artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: scraper-results-${{ github.run_number }}
        path: |
          *.json
          *.csv
          debug_*.png
          debug_*.html
        retention-days: 30
    
    - name: Commit and push results (optional)
      if: success()
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Create data directory if it doesn't exist
        mkdir -p data
        
        # Move results to data directory
        mv indeed_cr_jobs_*.json data/ 2>/dev/null || true
        mv indeed_cr_jobs_*.csv data/ 2>/dev/null || true
        
        # Check if there are changes
        if [ -n "$(git status --porcelain)" ]; then
          git add data/
          git commit -m "Update scraped jobs - $(date +'%Y-%m-%d %H:%M:%S')"
          git push
        else
          echo "No changes to commit"
        fi
      continue-on-error: true
    
    - name: Create summary
      if: always()
      run: |
        echo "## Scraper Results ðŸ“Š" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Count files
        JSON_COUNT=$(ls -1 *.json 2>/dev/null | wc -l)
        CSV_COUNT=$(ls -1 *.csv 2>/dev/null | wc -l)
        
        echo "- JSON files created: $JSON_COUNT" >> $GITHUB_STEP_SUMMARY
        echo "- CSV files created: $CSV_COUNT" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # List files
        if [ $JSON_COUNT -gt 0 ]; then
          echo "### Generated Files:" >> $GITHUB_STEP_SUMMARY
          ls -lh *.json *.csv 2>/dev/null | awk '{print "- " $9 " (" $5 ")"}' >> $GITHUB_STEP_SUMMARY
        fi

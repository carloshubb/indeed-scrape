name: Weekly Indeed Scraper
on:
  schedule:
    - cron: "0 0 * * 1"   # Runs every Monday at 00:00 UTC
  workflow_dispatch:        # Allows manual trigger from GitHub UI

jobs:
  scrape-indeed:
    runs-on: ubuntu-latest
    steps:
      # 1. Checkout your repository
      - name: Checkout repository
        uses: actions/checkout@v4
      
      # 2. Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
      
      # 3. Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium webdriver-manager
      
      # 4. Set up Chrome and ChromeDriver
      - name: Set up Chrome
        uses: browser-actions/setup-chrome@latest
        with:
          chrome-version: stable
      
      # 5. Clean up any existing files
      - name: Clean up old files
        run: |
          rm -f indeed_cr_jobs*.csv indeed_cr_jobs*.json
          echo "Cleaned up any existing files"
      
      # 6. Run the Indeed scraper in headless mode
      - name: Run Indeed scraper
        env:
          PYTHONUNBUFFERED: 1
        run: |
          python - <<'EOF'
          from selenium import webdriver
          from selenium.webdriver.chrome.service import Service
          from selenium.webdriver.chrome.options import Options
          from webdriver_manager.chrome import ChromeDriverManager
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait
          from selenium.webdriver.support import expected_conditions as EC
          from datetime import datetime
          import json
          import csv
          import time
          import random
          import os
          
          # Clean up any existing files first
          for f in os.listdir('.'):
              if f.startswith('indeed_cr_jobs') and (f.endswith('.csv') or f.endswith('.json')):
                  os.remove(f)
                  print(f"Removed old file: {f}")
          
          # Set up Chrome options
          chrome_options = Options()
          chrome_options.add_argument('--headless=new')
          chrome_options.add_argument('--no-sandbox')
          chrome_options.add_argument('--disable-dev-shm-usage')
          chrome_options.add_argument('--disable-gpu')
          chrome_options.add_argument('--disable-blink-features=AutomationControlled')
          chrome_options.add_argument('--window-size=1920,1080')
          chrome_options.add_argument('--lang=es-ES')
          chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
          chrome_options.add_experimental_option('useAutomationExtension', False)
          
          # Initialize driver
          service = Service(ChromeDriverManager().install())
          driver = webdriver.Chrome(service=service, options=chrome_options)
          driver.set_page_load_timeout(30)
          
          print("=" * 70)
          print("INDEED COSTA RICA JOB SCRAPER")
          print("=" * 70)
          print(f"Chrome version: {driver.capabilities['browserVersion']}")
          print(f"ChromeDriver version: {driver.capabilities['chrome']['chromedriverVersion'].split()[0]}")
          print("=" * 70)
          
          search_url = "https://cr.indeed.com/jobs?q=&l=costa+rica"
          all_jobs = []
          
          try:
              # Scrape 3 pages
              for page in range(3):
                  url = search_url if page == 0 else f"{search_url}&start={page * 10}"
                  print(f"\n📄 Scraping page {page + 1}/3...")
                  
                  driver.get(url)
                  time.sleep(random.uniform(3, 5))
                  
                  # Scroll to load content
                  driver.execute_script("window.scrollTo(0, document.body.scrollHeight/2);")
                  time.sleep(2)
                  
                  # Find job cards
                  job_cards = driver.find_elements(By.CSS_SELECTOR, 'div.job_seen_beacon, div[data-jk], td.resultContent')
                  print(f"  ✅ Found {len(job_cards)} job cards")
                  
                  for idx, card in enumerate(job_cards, 1):
                      try:
                          job_data = {
                              'title': None,
                              'company': None,
                              'location': None,
                              'salary': None,
                              'description': None,
                              'url': None,
                              'date_posted': None,
                              'job_type': None,
                              'scraped_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                          }
                          
                          # Extract title
                          try:
                              title_elem = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle span, h2.jobTitle a, span[id^="jobTitle-"]')
                              job_data['title'] = title_elem.text.strip()
                          except:
                              pass
                          
                          # Extract company
                          try:
                              company_elem = card.find_element(By.CSS_SELECTOR, 'span[data-testid="company-name"], .companyName')
                              job_data['company'] = company_elem.text.strip()
                          except:
                              pass
                          
                          # Extract location
                          try:
                              loc_elem = card.find_element(By.CSS_SELECTOR, 'div[data-testid="text-location"], .companyLocation')
                              job_data['location'] = loc_elem.text.strip()
                          except:
                              pass
                          
                          # Extract salary
                          try:
                              salary_elem = card.find_element(By.CSS_SELECTOR, '.salary-snippet-container, .salary-snippet, .estimated-salary')
                              job_data['salary'] = salary_elem.text.strip()
                          except:
                              pass
                          
                          # Extract description snippet
                          try:
                              snippet_elem = card.find_element(By.CSS_SELECTOR, '.job-snippet, ul.job-snippet, .job-snippet-text')
                              job_data['description'] = snippet_elem.text.strip()
                          except:
                              pass
                          
                          # Extract URL
                          try:
                              link = card.find_element(By.CSS_SELECTOR, 'h2.jobTitle a, a.jcs-JobTitle')
                              job_data['url'] = link.get_attribute('href')
                          except:
                              pass
                          
                          # Extract date
                          try:
                              date_elem = card.find_element(By.CSS_SELECTOR, 'span.date, .date')
                              job_data['date_posted'] = date_elem.text.strip()
                          except:
                              pass
                          
                          # Extract job type
                          try:
                              type_elem = card.find_element(By.CSS_SELECTOR, '.metadata, .job-snippet li')
                              type_text = type_elem.text.strip().lower()
                              if 'tiempo completo' in type_text or 'full-time' in type_text:
                                  job_data['job_type'] = 'Full-time'
                              elif 'medio tiempo' in type_text or 'part-time' in type_text:
                                  job_data['job_type'] = 'Part-time'
                              elif 'temporal' in type_text or 'contract' in type_text:
                                  job_data['job_type'] = 'Contract'
                          except:
                              pass
                          
                          if job_data['title']:
                              all_jobs.append(job_data)
                              print(f"    {idx:2d}. {job_data['title'][:60]}")
                          
                          time.sleep(random.uniform(0.3, 0.8))
                          
                      except Exception as e:
                          print(f"    ⚠️  Error extracting job {idx}: {str(e)[:50]}")
                          continue
                  
                  # Delay between pages
                  if page < 2:
                      delay = random.uniform(4, 7)
                      print(f"  ⏳ Waiting {delay:.1f}s before next page...")
                      time.sleep(delay)
              
              print("\n" + "=" * 70)
              print(f"✅ Successfully scraped {len(all_jobs)} jobs!")
              print("=" * 70)
              
              # Save to SINGLE CSV file with fixed name
              csv_file = 'indeed_cr_jobs.csv'
              json_file = 'indeed_cr_jobs.json'
              
              if all_jobs:
                  # Save CSV
                  keys = all_jobs[0].keys()
                  with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                      writer = csv.DictWriter(f, fieldnames=keys)
                      writer.writeheader()
                      writer.writerows(all_jobs)
                  print(f"\n💾 Saved {len(all_jobs)} jobs to: {csv_file}")
                  
                  # Save JSON
                  with open(json_file, 'w', encoding='utf-8') as f:
                      json.dump(all_jobs, f, ensure_ascii=False, indent=2)
                  print(f"💾 Saved {len(all_jobs)} jobs to: {json_file}")
                  
                  # Print sample
                  print("\n" + "=" * 70)
                  print("SAMPLE JOB:")
                  print("=" * 70)
                  sample = all_jobs[0]
                  for key, value in sample.items():
                      if value:
                          print(f"{key:15s}: {str(value)[:80]}")
                  print("=" * 70)
              else:
                  print("\n⚠️  No jobs were scraped!")
              
          except Exception as e:
              print(f"\n❌ Error during scraping: {e}")
              import traceback
              traceback.print_exc()
          
          finally:
              driver.quit()
              print("\n🔒 Browser closed.")
          
          EOF
      
      # 7. Verify single file exists
      - name: Verify output
        run: |
          echo "Files in directory:"
          ls -lh indeed_cr_jobs.*
          echo ""
          echo "CSV file contents (first 5 lines):"
          head -5 indeed_cr_jobs.csv || echo "No CSV file found"
      
      # 8. Upload ONLY the single CSV and JSON file
      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: indeed-scraped-data-${{ github.run_number }}
          path: |
            indeed_cr_jobs.csv
            indeed_cr_jobs.json
          retention-days: 90
